{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2MQhpDMNemqwLN5KYzc6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsPronay/HSIC/blob/main/MVit_single_ai_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/itsPronay/MViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP3LK-h1Rg8d",
        "outputId": "5d168a0b-02d6-4bf9-d9ca-525ac0878c34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MViT'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 98 (delta 28), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 2.59 MiB | 16.92 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), 'MViT'))\n",
        "\n",
        "import time\n",
        "import math\n",
        "import argparse\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from einops import rearrange, repeat\n",
        "from timm.models.vision_transformer import Block\n",
        "\n",
        "\n",
        "from data_prepare import mirror_hsi\n",
        "from data_prepare import choose_train_and_test\n",
        "from data_prepare import choose_all_pixels, all_data\n",
        "from data_prepare import train_and_test_data, train_and_test_label\n",
        "\n",
        "\n",
        "from Utils import output_metric, plot_confusion_matrix\n",
        "from CNNUtils import train, test, valid\n",
        "from Utils import list_to_colormap, classification_map, print_args\n",
        "from Utils import ActivationOutputData\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Cw4xa1yVaKE0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "!curl -L -o pavia-university-hsi.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/syamkakarla/pavia-university-hsi && unzip pavia-university-hsi.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1xA6iIN7qoH",
        "outputId": "3d10b337-c005-48be-f74d-1e0a9ce74ee6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 33.2M  100 33.2M    0     0  7933k      0  0:00:04  0:00:04 --:--:-- 11.6M\n",
            "Archive:  pavia-university-hsi.zip\n",
            "  inflating: PaviaU.mat              \n",
            "  inflating: PaviaU_gt.mat           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'train_dir': 'PaviaU.mat',\n",
        "    'ground_truth' : 'PaviaU_gt.mat'\n",
        "}\n"
      ],
      "metadata": {
        "id": "nekqX2eYDhC2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser('Pavia_HSIC')\n",
        "\n",
        "parser.add_argument('--epoch', type=int, default=10)\n",
        "parser.add_argument('--learning_rate', type=float, default=0.001)\n",
        "parser.add_argument('--batch_size', type=int, default=10)\n",
        "parser.add_argument('--patch_size', type=int, default=15)\n",
        "parser.add_argument('--seed', type=int, default=41)\n",
        "parser.add_argument('--train_number', type=int, default=25, help='num_train_per_class')\n",
        "parser.add_argument('--gamma', type=float, default=0.99, help='gamma')\n",
        "parser.add_argument('--weight_decay', type=float, default=0.001, help='weight_decay')\n",
        "\n",
        "args, _ = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "HKfHUjV1DXyd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = os.path.join(os.getcwd(), config['train_dir'])\n",
        "ground_truth_path = os.path.join(os.getcwd(), config['ground_truth'])\n",
        "\n",
        "dataset = loadmat(dataset_path)['paviaU']\n",
        "ground_truth = loadmat(ground_truth_path)['paviaU_gt']\n",
        "\n",
        "print('Training shape', str(dataset.shape))\n",
        "print('GT shape', str(ground_truth.shape))\n",
        "\n",
        "classes = np.max(ground_truth)\n",
        "print('Number of classes in Pavia dartaset are ' + str(classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2G5ORZC8FG1",
        "outputId": "cd32aca3-9a81-445a-e047-29aa4703a2fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shape (610, 340, 103)\n",
            "GT shape (610, 340)\n",
            "Number of classes in Pavia dartaset are 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalizing\n",
        "shapeor = dataset.shape\n",
        "\n",
        "dataset = dataset.reshape(np.prod(dataset.shape[:2]), np.prod(dataset.shape[2:]))\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "std_data = std_scaler.fit_transform(dataset)\n",
        "dataset = std_data.reshape(shapeor)\n"
      ],
      "metadata": {
        "id": "JNVJldAWNLOo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def applyPCA(data, numComponents=30):\n",
        "    new_data = np.reshape(data, (-1, data.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    new_data = pca.fit_transform(new_data)\n",
        "    new_data = np.reshape(new_data, (data.shape[0], data.shape[1], numComponents))\n",
        "    return new_data, pca"
      ],
      "metadata": {
        "id": "GCcGo80hOycS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 30\n",
        "print('Before PCA size ' + str(dataset.shape))\n",
        "dataset, pca = applyPCA(dataset, numComponents=K)\n",
        "print('After PCA size ' + str(dataset.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLQxLRuwHLrH",
        "outputId": "74f7a5b8-d89e-4e0e-fd01-916ac8da5235"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before PCA size (610, 340, 103)\n",
            "After PCA size (610, 340, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "height, width, band = dataset.shape\n",
        "\n",
        "mirror_data = mirror_hsi(height, width, band, dataset, patch_size=args.patch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tow6oDKQUM_e",
        "outputId": "82d2d990-f6c6-4334-8ca4-929ea6171c45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************\n",
            "patch_size : 15\n",
            "mirror_data shape : [624, 354, 30]\n",
            "*******************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_pos_train, total_pos_test, total_pos_valid, number_train, number_test, number_valid = choose_train_and_test(ground_truth, args.train_number, args.seed)\n",
        "\n",
        "x_train, x_test, x_valid = train_and_test_data(mirror_data, band, total_pos_train, total_pos_test, total_pos_valid, args.patch_size)\n",
        "y_train, y_test, y_valid = train_and_test_label(number_train, number_test, number_valid, classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmv_QHscUsUl",
        "outputId": "45c13a97-6306-4a02-8ed9-a0d09af5cfc1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************\n",
            "x_train shape = (225, 15, 15, 30), type = float64\n",
            "x_test  shape = (42551, 15, 15, 30), type = float64\n",
            "x_valid  shape = (900, 15, 15, 30), type = float64\n",
            "*******************************************************\n",
            "y_train: shape = (225,), type = int64\n",
            "y_test: shape = (42551,), type = int64\n",
            "y_valid: shape = (900,), type = int64\n",
            "*******************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_pos_train[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcKqwS4QVpZt",
        "outputId": "d630f366-e630-42ab-ea56-2e502f7a6942"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 91, 145],\n",
              "       [271, 228],\n",
              "       [ 57, 131],\n",
              "       [168, 184],\n",
              "       [570,  34],\n",
              "       [582,  68],\n",
              "       [  4,  98],\n",
              "       [ 22,  87],\n",
              "       [ 48,  41],\n",
              "       [ 98, 147]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as Data\n",
        "\n",
        "# load data\n",
        "x_train = torch.from_numpy(x_train.transpose(0, 3, 1, 2)).unsqueeze(1).type(torch.FloatTensor)  # (90, 30, 15, 15)\n",
        "print(x_train.shape)\n",
        "y_train = torch.from_numpy(y_train).type(torch.LongTensor)  # (13,)\n",
        "train_label = Data.TensorDataset(x_train, y_train)\n",
        "\n",
        "x_test = torch.from_numpy(x_test.transpose(0, 3, 1, 2)).unsqueeze(1).type(torch.FloatTensor)  # (5198, 30, 15, 15)\n",
        "print(x_test.shape)\n",
        "y_test = torch.from_numpy(y_test).type(torch.LongTensor)  # (5198,)\n",
        "test_label = Data.TensorDataset(x_test, y_test)\n",
        "\n",
        "x_valid = torch.from_numpy(x_valid.transpose(0, 3, 1, 2)).unsqueeze(1).type(torch.FloatTensor)  # (5211, 30, 15, 15)\n",
        "print(x_valid.shape)\n",
        "y_valid = torch.from_numpy(y_valid).type(torch.LongTensor)\n",
        "valid_label = Data.TensorDataset(x_valid, y_valid)\n",
        "\n",
        "train_loader = Data.DataLoader(train_label, batch_size=30, shuffle=True)\n",
        "test_loader = Data.DataLoader(test_label, batch_size=128, shuffle=True)\n",
        "valid_loader = Data.DataLoader(valid_label, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGQHqt_QWI5J",
        "outputId": "0b93c748-a278-45dc-f1a1-271244985353"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([225, 1, 30, 15, 15])\n",
            "torch.Size([42551, 1, 30, 15, 15])\n",
            "torch.Size([900, 1, 30, 15, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "uUnDgI0oWVGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M, )\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2, )  32\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M, )  169\n",
        "    out = np.einsum('m, d -> md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=True):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)  # (H*W, D)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)  # (1+H*W, D)\n",
        "    return pos_embed"
      ],
      "metadata": {
        "id": "bWC1UzGtWQR8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, repeat\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import Block\n",
        "\n",
        "class MViT(nn.Module):\n",
        "    def __init__(self, in_chans=1, bands=30, num_classes=9, dim=64, heads=4, depth=3, dropout=0.2):\n",
        "        super(MViT, self).__init__()\n",
        "        self.conv3d = nn.Sequential(nn.Conv3d(1, 8, 3), nn.BatchNorm3d(8), nn.ReLU())\n",
        "        self.conv2d = nn.Sequential(nn.Conv2d(8*28, 64, 3), nn.BatchNorm2d(64), nn.ReLU())\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
        "        nn.init.normal_(self.cls_token, std=.02)\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 121 + 1, dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(dim, 11, cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        self.blocks = nn.ModuleList([Block(dim, heads, qkv_bias=True, attn_drop=0.1) for _ in range(depth)])\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.cls_head = nn.Linear(dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def random_masking(self, x, mask_ratio=0.75):\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]  (N, L)\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # 从小到大排序，返回索引 (N, L)\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "        return x_masked\n",
        "\n",
        "    def forward(self, x, mask_ratio=0.75):\n",
        "        # (B, 1, 30, 15, 15)\n",
        "        x = self.conv3d(x)  # (B, 8, 28, 13, 13)\n",
        "        x = rearrange(x, 'b c d h w -> b (c d) h w')  # (B, 224, 13, 13)\n",
        "        x = self.conv2d(x)  # (B, 64, 11, 11)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')  # (B, 121, 64)\n",
        "\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        center_embed = x[:, 60, :].unsqueeze(1)  # (B, 1, 64)\n",
        "        x = torch.cat([x[:, :60, :], x[:, 61:, :]], dim=1)  # (B, 120, 64)\n",
        "\n",
        "        if self.training:\n",
        "            x = self.random_masking(x, mask_ratio)  # (B, 30, 64)\n",
        "\n",
        "        x = torch.cat([center_embed, x], dim=1)\n",
        "\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_token = repeat(cls_token, '1 n d -> b n d', b = x.shape[0])  # (B, 1, 64)\n",
        "\n",
        "        x = torch.cat((cls_token, x), dim = 1)\n",
        "\n",
        "        x = self.dropout(x)  # (B, 31, 64)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        latent = self.norm(x[:, 0, :])\n",
        "        x = self.cls_head(latent)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GFhuRs-RWXEj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MViT(num_classes=classes).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.gamma)"
      ],
      "metadata": {
        "id": "_OP3vOzLW8mM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('start training')\n",
        "acc_list = [0.00]\n",
        "path = 'mvit.pt'\n",
        "tic = time.time()\n",
        "for epoch in range(args.epoch):\n",
        "    # 计算的是移动平均准确率\n",
        "    train_acc, train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    valid_acc, valid_loss = valid(model, valid_loader, criterion)\n",
        "    print(\"Epoch: {:03d} - train_loss: {:.4f} - train_acc: {:.4f} - valid_loss: {:.4f} - valid_acc: {:.4f}\".\\\n",
        "          format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
        "    scheduler.step()\n",
        "\n",
        "    acc_list.append(valid_acc)\n",
        "    if acc_list[-1] > acc_list[-2]:\n",
        "        print(\"val_acc improved from {:.4f} to {:.4f}, saving model to mvit.pt\".format(acc_list[-2], acc_list[-1]))\n",
        "        torch.save(model.state_dict(), path)\n",
        "    else:\n",
        "        print(\"val_acc did not improve from {:.4f}\".format(acc_list[-2]))\n",
        "        acc_list[-1] = acc_list[-2]\n",
        "\n",
        "toc = time.time()\n",
        "print(\"Running Time: {:.2f}\".format(toc-tic))\n",
        "print(\"**************************************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a-kVRD3XIl3",
        "outputId": "3d22998e-df50-40b0-8cc5-ed1f77ce1116"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training\n",
            "Epoch: 001 - train_loss: 1.6653 - train_acc: 39.1111 - valid_loss: 1.3176 - valid_acc: 64.0000\n",
            "val_acc improved from 0.0000 to 64.0000, saving model to mvit.pt\n",
            "Epoch: 002 - train_loss: 0.8390 - train_acc: 76.0000 - valid_loss: 0.6720 - valid_acc: 83.8889\n",
            "val_acc improved from 64.0000 to 83.8889, saving model to mvit.pt\n",
            "Epoch: 003 - train_loss: 0.5940 - train_acc: 83.1111 - valid_loss: 0.5579 - valid_acc: 78.3333\n",
            "val_acc did not improve from 83.8889\n",
            "Epoch: 004 - train_loss: 0.4816 - train_acc: 87.5556 - valid_loss: 0.4277 - valid_acc: 88.5556\n",
            "val_acc improved from 83.8889 to 88.5556, saving model to mvit.pt\n",
            "Epoch: 005 - train_loss: 0.3766 - train_acc: 89.3333 - valid_loss: 0.3887 - valid_acc: 89.1111\n",
            "val_acc improved from 88.5556 to 89.1111, saving model to mvit.pt\n",
            "Epoch: 006 - train_loss: 0.2749 - train_acc: 92.4445 - valid_loss: 0.3443 - valid_acc: 88.5556\n",
            "val_acc did not improve from 89.1111\n",
            "Epoch: 007 - train_loss: 0.1945 - train_acc: 96.8889 - valid_loss: 0.2336 - valid_acc: 95.5556\n",
            "val_acc improved from 89.1111 to 95.5556, saving model to mvit.pt\n",
            "Epoch: 008 - train_loss: 0.1477 - train_acc: 97.7778 - valid_loss: 0.1952 - valid_acc: 95.4445\n",
            "val_acc did not improve from 95.5556\n",
            "Epoch: 009 - train_loss: 0.1387 - train_acc: 97.3333 - valid_loss: 0.1776 - valid_acc: 95.5556\n",
            "val_acc did not improve from 95.5556\n",
            "Epoch: 010 - train_loss: 0.0908 - train_acc: 99.5556 - valid_loss: 0.1851 - valid_acc: 95.4445\n",
            "val_acc did not improve from 95.5556\n",
            "Running Time: 2.83\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()\n",
        "\n",
        "tar_test, pre_test = test(model, test_loader)\n",
        "OA_test, AA_mean_test, Kappa_test, AA_test = output_metric(tar_test, pre_test)\n",
        "AA_test = np.around(AA_test*100, 2)"
      ],
      "metadata": {
        "id": "d5aWzaHDYTHt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"*******************************************************************\")\n",
        "print(\"Final result:\")\n",
        "print(\"OA: {:.2f}, AA: {:.2f}, Kappa: {:.4f}\".format(OA_test * 100., AA_mean_test*100., Kappa_test))\n",
        "print(\"*******************************************************************\")\n",
        "print(\"Recal: {}\".format(AA_test))\n",
        "print(\"*******************************************************************\")\n",
        "print(\"Parameter:\")\n",
        "print_args(vars(args))\n",
        "print(\"*******************************************************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRY7EAPiYoRI",
        "outputId": "b85ff506-c7f4-478e-bd39-20dd3468247e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************\n",
            "Final result:\n",
            "OA: 94.59, AA: 94.40, Kappa: 0.9286\n",
            "*******************************************************************\n",
            "Recal: [ 91.31  96.28  90.74  90.79 100.    99.94  98.54  86.22  95.77]\n",
            "*******************************************************************\n",
            "Parameter:\n",
            "epoch: 10\n",
            "learning_rate: 0.001\n",
            "batch_size: 10\n",
            "patch_size: 15\n",
            "seed: 41\n",
            "train_number: 25\n",
            "gamma: 0.99\n",
            "weight_decay: 0.001\n",
            "*******************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "*******************************************************************\n",
        "Final result:\n",
        "OA: 91.92, AA: 93.49, Kappa: 0.8947\n",
        "*******************************************************************\n",
        "Recal: [ 88.83  90.8   86.02  89.47 100.   100.    99.46  90.84  95.99]\n",
        "*******************************************************************\n",
        "Parameter:\n",
        "epoch: 300\n",
        "learning_rate: 0.001\n",
        "batch_size: 10\n",
        "patch_size: 15\n",
        "seed: 41\n",
        "train_number: 25\n",
        "gamma: 0.99\n",
        "weight_decay: 0.001\n",
        "*******************************************************************```\n",
        "\n"
      ],
      "metadata": {
        "id": "J4C77laSc5cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"qai-hub[torch]\"\n",
        "!qai-hub configure --api_token znlq94irgqllstitbp39jzkdvvkk7cmu6lrcjy33"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJPdtJkVbwyM",
        "outputId": "f351a4c3-17b0-4334-8e16-c1ec2e2fa244"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qai-hub[torch]\n",
            "  Downloading qai_hub-0.44.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting backoff>=2.2 (from qai-hub[torch])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (2.1.0)\n",
            "Requirement already satisfied: h5py<4,>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (3.15.1)\n",
            "Requirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (26.0)\n",
            "Requirement already satisfied: prettytable>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (3.17.0)\n",
            "Requirement already satisfied: protobuf<=6.31.1,>=3.20 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (5.29.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (1.0.0)\n",
            "Collecting s3transfer<0.14,>=0.10.3 (from qai-hub[torch])\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting semver>=3.0 (from qai-hub[torch])\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (4.15.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision>=0.14 in /usr/local/lib/python3.12/dist-packages (from qai-hub[torch]) (0.24.0+cu128)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable>=3.9.0->qai-hub[torch]) (0.5.3)\n",
            "Collecting botocore<2.0a.0,>=1.37.4 (from s3transfer<0.14,>=0.10.3->qai-hub[torch])\n",
            "  Downloading botocore-1.42.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->qai-hub[torch]) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.14->qai-hub[torch]) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->qai-hub[torch]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->qai-hub[torch]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->qai-hub[torch]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->qai-hub[torch]) (2026.1.4)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<2.0a.0,>=1.37.4->s3transfer<0.14,>=0.10.3->qai-hub[torch])\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<2.0a.0,>=1.37.4->s3transfer<0.14,>=0.10.3->qai-hub[torch]) (2.9.0.post0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->qai-hub[torch]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->qai-hub[torch]) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0a.0,>=1.37.4->s3transfer<0.14,>=0.10.3->qai-hub[torch]) (1.17.0)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading qai_hub-0.44.0-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.49-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: semver, jmespath, backoff, botocore, s3transfer, qai-hub\n",
            "Successfully installed backoff-2.2.1 botocore-1.42.49 jmespath-1.1.0 qai-hub-0.44.0 s3transfer-0.13.1 semver-3.0.4\n",
            "2026-02-14 23:46:40.936 - INFO - Enabling verbose logging.\n",
            "qai-hub configuration saved to /root/.qai_hub/client.ini\n",
            "==================== /root/.qai_hub/client.ini ====================\n",
            "[api]\n",
            "api_token = znlq94irgqllstitbp39jzkdvvkk7cmu6lrcjy33\n",
            "api_url = https://workbench.aihub.qualcomm.com\n",
            "web_url = https://workbench.aihub.qualcomm.com\n",
            "verbose = True\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(path))\n",
        "model = model.to('cpu').eval()\n",
        "print('Model moved to cpu and in evaluation mode')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddhr5RgWeMIm",
        "outputId": "6e308491-a85d-4fe4-a546-506e48a1f3ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model moved to cpu and in evaluation mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import qai_hub as hub\n",
        "\n",
        "devices = [\n",
        "    # hub.Device('Dragonwing IQ-9075 EVK'),\n",
        "    # hub.Device('QCS8550 (Proxy)'),\n",
        "    # hub.Device('Google Pixel 10 Pro XL'),\n",
        "    # hub.Device('Samsung Galaxy S24 (Family)'),\n",
        "    hub.Device('Samsung Galaxy S24 Ultra')\n",
        "]"
      ],
      "metadata": {
        "id": "Pu8b3kt6eQt7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "traced_models = []\n",
        "\n",
        "shape = (1, 1, 30, 15, 15)\n",
        "\n",
        "input_shape: tuple[int, ...] = shape\n",
        "example_input = torch.rand(input_shape)\n",
        "\n",
        "model_name = 'traced_MVIT'\n",
        "traced_model = torch.jit.trace(model, example_input)"
      ],
      "metadata": {
        "id": "ED70wXJ4ehSw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "compile_jobs = []\n",
        "\n",
        "for device in devices:\n",
        "    name_formatted =  \"MViT_\" + device.name\n",
        "    print(\"Submitting compile job for: \" + name_formatted)\n",
        "\n",
        "    job = hub.submit_compile_job(\n",
        "        model=traced_model,\n",
        "        name=name_formatted,\n",
        "        device=device,\n",
        "        input_specs=dict(image=input_shape),\n",
        "    )\n",
        "    assert isinstance(job, hub.CompileJob)\n",
        "    compile_jobs.append((name_formatted, job))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxyuOow1gfa_",
        "outputId": "3a399ce2-34db-434d-9555-88ab9acab333"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitting compile job for: MViT_Samsung Galaxy S24 Ultra\n",
            "Uploading tmpttm7ksi4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 1.22M/1.22M [00:01<00:00, 838kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduled compile job (jgdvz3jzg) successfully. To see the status and results:\n",
            "    https://workbench.aihub.qualcomm.com/jobs/jgdvz3jzg/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "profile_jobs = []\n",
        "\n",
        "for name, job in compile_jobs:\n",
        "    device = job.device\n",
        "\n",
        "    print(\"Submitting profiling job for:\" + name)\n",
        "\n",
        "    pf_job = hub.submit_profile_job(\n",
        "        model=job.get_target_model(),\n",
        "        device=device,\n",
        "        name=job.name + \"_profiling\"\n",
        "    )\n",
        "\n",
        "    assert isinstance(pf_job, hub.ProfileJob)\n",
        "    profile_jobs.append((name, pf_job))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1ZmmCCGeo9d",
        "outputId": "86b10e51-c3f8-43c8-9617-1cba5b97a269"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitting profiling job for:MViT_Samsung Galaxy S24 Ultra\n",
            "Scheduled profile job (jp18vjrng) successfully. To see the status and results:\n",
            "    https://workbench.aihub.qualcomm.com/jobs/jp18vjrng/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import qai_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Extract and Display Results\n",
        "# ============================================================================\n",
        "\n",
        "def us_to_ms(x):\n",
        "    \"\"\"Convert microseconds to milliseconds\"\"\"\n",
        "    return x / 1e3\n",
        "\n",
        "def bytes_to_mb(x):\n",
        "    \"\"\"Convert bytes to megabytes\"\"\"\n",
        "    return x / (1024 ** 2)\n",
        "\n",
        "def extract_architecture(model_name):\n",
        "    \"\"\"Extract architecture from model name\"\"\"\n",
        "    return model_name.split(\"_\")[0]\n",
        "\n",
        "summary_rows = []\n",
        "util_rows = []\n",
        "memory_rows = []\n",
        "bottleneck_rows = []\n",
        "\n",
        "for name, pf_job in profile_jobs:\n",
        "    result = pf_job.download_profile()\n",
        "    s = result[\"execution_summary\"]\n",
        "    d = pd.DataFrame(result[\"execution_detail\"])\n",
        "    times = np.array(s[\"all_inference_times\"])\n",
        "\n",
        "    model_name = name\n",
        "    device_name = pf_job.device.name\n",
        "    architecture = extract_architecture(model_name)\n",
        "\n",
        "    # For HSI model, resolution is fixed at 15x15 patches with 30 bands\n",
        "    resolution = \"15x15x30\"  # Height x Width x Bands\n",
        "\n",
        "    # -------------------------------\n",
        "    # Table 1: End-to-End Performance\n",
        "    # -------------------------------\n",
        "    summary_rows.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Input Size\": resolution,\n",
        "        \"Device\": device_name,\n",
        "        \"Mean Latency (ms)\": round(us_to_ms(times.mean()), 4),\n",
        "        \"Median Latency (ms)\": round(us_to_ms(np.median(times)), 4),\n",
        "        \"P95 Latency (ms)\": round(us_to_ms(np.percentile(times, 95)), 4),\n",
        "        \"P99 Latency (ms)\": round(us_to_ms(np.percentile(times, 99)), 4),\n",
        "        \"Std Dev (ms)\": round(us_to_ms(times.std()), 4),\n",
        "        \"Cold Start (ms)\": round(us_to_ms(s[\"first_load_time\"]), 4),\n",
        "        \"Warm Start (ms)\": round(us_to_ms(s[\"warm_load_time\"]), 4),\n",
        "    })\n",
        "\n",
        "    # -------------------------------\n",
        "    # Table 2: Memory Footprint\n",
        "    # -------------------------------\n",
        "    memory_rows.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Device\": device_name,\n",
        "        \"Inference Peak (MB)\": round(bytes_to_mb(s[\"estimated_inference_peak_memory\"]), 2),\n",
        "        \"Cold Start Peak (MB)\": round(bytes_to_mb(s[\"first_load_peak_memory\"]), 2),\n",
        "        \"Warm Start Peak (MB)\": round(bytes_to_mb(s[\"warm_load_peak_memory\"]), 2),\n",
        "    })\n",
        "\n",
        "    # -------------------------------\n",
        "    # Table 3: Accelerator Utilization\n",
        "    # -------------------------------\n",
        "    total_time = d[\"execution_time\"].sum()\n",
        "    util = d.groupby(\"compute_unit\")[\"execution_time\"].sum() / total_time * 100\n",
        "\n",
        "    util_rows.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Device\": device_name,\n",
        "        \"CPU (%)\": round(util.get(\"CPU\", 0.0), 2),\n",
        "        \"GPU (%)\": round(util.get(\"GPU\", 0.0), 2),\n",
        "        \"NPU (%)\": round(util.get(\"NPU\", 0.0), 2),\n",
        "        \"Total Time (ms)\": round(us_to_ms(total_time), 2),\n",
        "        \"Dominant Unit\": util.idxmax() if len(util) > 0 else \"N/A\",\n",
        "    })\n",
        "\n",
        "    # -------------------------------\n",
        "    # Table 4: Performance Bottlenecks\n",
        "    # -------------------------------\n",
        "    top_ops = d.nlargest(5, \"execution_time\")[[\"name\", \"type\", \"compute_unit\", \"execution_time\"]]\n",
        "\n",
        "    bottleneck_rows.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Device\": device_name,\n",
        "        \"Slowest Op\": top_ops.iloc[0][\"name\"].split(\"/\")[-1] if len(top_ops) > 0 else \"N/A\",\n",
        "        \"Op Type\": top_ops.iloc[0][\"type\"] if len(top_ops) > 0 else \"N/A\",\n",
        "        \"Op Time (ms)\": round(us_to_ms(top_ops.iloc[0][\"execution_time\"]), 4) if len(top_ops) > 0 else 0,\n",
        "        \"Op Unit\": top_ops.iloc[0][\"compute_unit\"] if len(top_ops) > 0 else \"N/A\",\n",
        "        \"Top 5 Ops Time (ms)\": round(us_to_ms(top_ops[\"execution_time\"].sum()), 2),\n",
        "        \"% of Total\": round(top_ops[\"execution_time\"].sum() / total_time * 100, 2),\n",
        "    })\n",
        "\n",
        "# Create DataFrames\n",
        "table_perf = pd.DataFrame(summary_rows)\n",
        "table_mem = pd.DataFrame(memory_rows)\n",
        "table_util = pd.DataFrame(util_rows)\n",
        "table_bottleneck = pd.DataFrame(bottleneck_rows)\n",
        "\n",
        "# Display tables\n",
        "print(\"\\n\" + \"=\"*140)\n",
        "print(\"TABLE 1: End-to-End Performance\")\n",
        "print(\"=\"*140)\n",
        "print(table_perf.to_markdown(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*140)\n",
        "print(\"TABLE 2: Memory Footprint\")\n",
        "print(\"=\"*140)\n",
        "print(table_mem.to_markdown(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*140)\n",
        "print(\"TABLE 3: Accelerator Utilization\")\n",
        "print(\"=\"*140)\n",
        "print(table_util.to_markdown(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*140)\n",
        "print(\"TABLE 4: Performance Bottlenecks\")\n",
        "print(\"=\"*140)\n",
        "print(table_bottleneck.to_markdown(index=False))\n",
        "\n",
        "print(f\"\\n✅ Total profile jobs: {len(profile_jobs)}\")\n",
        "print(f\"✅ Models tested: {table_perf['Model'].unique().tolist()}\")\n",
        "print(f\"✅ Devices tested: {table_perf['Device'].unique().tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4FE8SxPhCPc",
        "outputId": "94dcf03b-dc23-4f66-e36a-69756777d70a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for profile job (jp18vjrng) completion. Type Ctrl+C to stop waiting at any time.\n",
            "    ✅ SUCCESS                          \u0007\n",
            "\n",
            "============================================================================================================================================\n",
            "TABLE 1: End-to-End Performance\n",
            "============================================================================================================================================\n",
            "| Model                         | Input Size   | Device                   |   Mean Latency (ms) |   Median Latency (ms) |   P95 Latency (ms) |   P99 Latency (ms) |   Std Dev (ms) |   Cold Start (ms) |   Warm Start (ms) |\n",
            "|:------------------------------|:-------------|:-------------------------|--------------------:|----------------------:|-------------------:|-------------------:|---------------:|------------------:|------------------:|\n",
            "| MViT_Samsung Galaxy S24 Ultra | 15x15x30     | Samsung Galaxy S24 Ultra |              0.3236 |                 0.275 |             0.3311 |             0.5297 |         0.3376 |           436.078 |           144.507 |\n",
            "\n",
            "============================================================================================================================================\n",
            "TABLE 2: Memory Footprint\n",
            "============================================================================================================================================\n",
            "| Model                         | Device                   |   Inference Peak (MB) |   Cold Start Peak (MB) |   Warm Start Peak (MB) |\n",
            "|:------------------------------|:-------------------------|----------------------:|-----------------------:|-----------------------:|\n",
            "| MViT_Samsung Galaxy S24 Ultra | Samsung Galaxy S24 Ultra |                134.18 |                 132.29 |                 136.78 |\n",
            "\n",
            "============================================================================================================================================\n",
            "TABLE 3: Accelerator Utilization\n",
            "============================================================================================================================================\n",
            "| Model                         | Device                   |   CPU (%) |   GPU (%) |   NPU (%) |   Total Time (ms) | Dominant Unit   |\n",
            "|:------------------------------|:-------------------------|----------:|----------:|----------:|------------------:|:----------------|\n",
            "| MViT_Samsung Galaxy S24 Ultra | Samsung Galaxy S24 Ultra |         0 |         0 |       100 |              0.51 | NPU             |\n",
            "\n",
            "============================================================================================================================================\n",
            "TABLE 4: Performance Bottlenecks\n",
            "============================================================================================================================================\n",
            "| Model                         | Device                   | Slowest Op   | Op Type   |   Op Time (ms) | Op Unit   |   Top 5 Ops Time (ms) |   % of Total |\n",
            "|:------------------------------|:-------------------------|:-------------|:----------|---------------:|:----------|----------------------:|-------------:|\n",
            "| MViT_Samsung Galaxy S24 Ultra | Samsung Galaxy S24 Ultra | Add          | ADD       |          0.103 | NPU       |                   0.3 |        58.15 |\n",
            "\n",
            "✅ Total profile jobs: 1\n",
            "✅ Models tested: ['MViT_Samsung Galaxy S24 Ultra']\n",
            "✅ Devices tested: ['Samsung Galaxy S24 Ultra']\n"
          ]
        }
      ]
    }
  ]
}